{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_MNIST.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/honggi82/computer_vision/blob/main/CNN_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpSzISkFc0xk",
        "outputId": "0f162934-c79e-4f7b-d5cf-b0153883e2c9"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
        "\n",
        "# MNIST data\n",
        "(train_X, train_Y), (test_X, test_Y) = mnist.load_data()\n",
        "\n",
        "print(train_X.shape,  train_Y.shape,  test_X.shape,  test_Y.shape)\n",
        "\n",
        "train_X = train_X.reshape(train_X.shape[0], 28,28,1).astype('float32')/255.0\n",
        "test_X = test_X.reshape(test_X.shape[0], 28,28,1).astype('float32')/255.0\n",
        "\n",
        "train_Y = to_categorical(train_Y) # One-Hot Encoding\n",
        "test_Y = to_categorical(test_Y) # One-Hot Encoding\n",
        "\n",
        "# Model\n",
        "model = Sequential([\n",
        "Conv2D(32, (3,3), padding='same', strides=(1,1), activation='relu', input_shape=(28,28,1)),\n",
        "MaxPooling2D(pool_size=(2,2)),\n",
        "Conv2D(64, (3,3), padding='same', strides=(1,1), activation='relu'),\n",
        "MaxPooling2D(pool_size=(2,2)),\n",
        "Flatten(),\n",
        "Dropout(0.5),\n",
        "Dense(10, activation='softmax')])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Training\n",
        "model.fit(train_X, train_Y, epochs=25)\n",
        "# Testing\n",
        "_, accuracy = model.evaluate(test_X, test_Y)\n",
        "print('Accuracy: ', accuracy)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_1 (Conv2D)           (None, 28, 28, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 14, 14, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 14, 14, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 7, 7, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3136)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 3136)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                31370     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 50186 (196.04 KB)\n",
            "Trainable params: 50186 (196.04 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "1875/1875 [==============================] - 73s 39ms/step - loss: 0.6570 - accuracy: 0.7947\n",
            "Epoch 2/25\n",
            "1875/1875 [==============================] - 74s 39ms/step - loss: 0.2125 - accuracy: 0.9356\n",
            "Epoch 3/25\n",
            "1875/1875 [==============================] - 74s 39ms/step - loss: 0.1611 - accuracy: 0.9514\n",
            "Epoch 4/25\n",
            "1875/1875 [==============================] - 71s 38ms/step - loss: 0.1367 - accuracy: 0.9582\n",
            "Epoch 5/25\n",
            "1875/1875 [==============================] - 72s 38ms/step - loss: 0.1200 - accuracy: 0.9635\n",
            "Epoch 6/25\n",
            "1875/1875 [==============================] - 71s 38ms/step - loss: 0.1093 - accuracy: 0.9662\n",
            "Epoch 7/25\n",
            "1875/1875 [==============================] - 74s 39ms/step - loss: 0.0995 - accuracy: 0.9701\n",
            "Epoch 8/25\n",
            "1875/1875 [==============================] - 71s 38ms/step - loss: 0.0927 - accuracy: 0.9716\n",
            "Epoch 9/25\n",
            "1875/1875 [==============================] - 72s 38ms/step - loss: 0.0879 - accuracy: 0.9736\n",
            "Epoch 10/25\n",
            "1875/1875 [==============================] - 71s 38ms/step - loss: 0.0813 - accuracy: 0.9747\n",
            "Epoch 11/25\n",
            "1875/1875 [==============================] - 71s 38ms/step - loss: 0.0794 - accuracy: 0.9756\n",
            "Epoch 12/25\n",
            "1875/1875 [==============================] - 72s 38ms/step - loss: 0.0758 - accuracy: 0.9766\n",
            "Epoch 13/25\n",
            "1875/1875 [==============================] - 71s 38ms/step - loss: 0.0723 - accuracy: 0.9780\n",
            "Epoch 14/25\n",
            "1875/1875 [==============================] - 72s 38ms/step - loss: 0.0708 - accuracy: 0.9783\n",
            "Epoch 15/25\n",
            "1875/1875 [==============================] - 73s 39ms/step - loss: 0.0667 - accuracy: 0.9797\n",
            "Epoch 16/25\n",
            "1875/1875 [==============================] - 72s 38ms/step - loss: 0.0662 - accuracy: 0.9797\n",
            "Epoch 17/25\n",
            "1875/1875 [==============================] - 71s 38ms/step - loss: 0.0631 - accuracy: 0.9804\n",
            "Epoch 18/25\n",
            "1875/1875 [==============================] - 72s 38ms/step - loss: 0.0631 - accuracy: 0.9801\n",
            "Epoch 19/25\n",
            "1875/1875 [==============================] - 71s 38ms/step - loss: 0.0606 - accuracy: 0.9805\n",
            "Epoch 20/25\n",
            "1875/1875 [==============================] - 70s 38ms/step - loss: 0.0597 - accuracy: 0.9813\n",
            "Epoch 21/25\n",
            "1875/1875 [==============================] - 72s 38ms/step - loss: 0.0585 - accuracy: 0.9817\n",
            "Epoch 22/25\n",
            "1875/1875 [==============================] - 70s 38ms/step - loss: 0.0580 - accuracy: 0.9815\n",
            "Epoch 23/25\n",
            "1875/1875 [==============================] - 74s 39ms/step - loss: 0.0549 - accuracy: 0.9831\n",
            "Epoch 24/25\n",
            "1875/1875 [==============================] - 70s 37ms/step - loss: 0.0548 - accuracy: 0.9826\n",
            "Epoch 25/25\n",
            "1875/1875 [==============================] - 70s 38ms/step - loss: 0.0522 - accuracy: 0.9840\n",
            "313/313 [==============================] - 4s 11ms/step - loss: 0.0339 - accuracy: 0.9887\n",
            "Accuracy:  0.9886999726295471\n"
          ]
        }
      ]
    }
  ]
}