{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_CIFAR.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPD9C2HmGXWDfahivIzuczH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/honggi82/computer_vision/blob/main/CNN_CIFAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7U5in5DMdf2E",
        "outputId": "8d50ef44-1baa-427b-9544-5918f12c86dc"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, pooling, Flatten, Dense\n",
        "\n",
        "# MNIST data\n",
        "(train_X, train_Y), (test_X, test_Y) = cifar10.load_data()\n",
        "print(train_X.shape, train_Y.shape, test_X.shape, test_Y.shape)\n",
        "\n",
        "train_X = train_X.reshape(train_X.shape[0], 32,32,3).astype('float32')/255.0\n",
        "test_X = test_X.reshape(test_X.shape[0], 32,32,3).astype('float32')/255.0\n",
        "\n",
        "train_Y = np_utils.to_categorical(train_Y) # One-Hot Encoding\n",
        "test_Y = np_utils.to_categorical(test_Y) # One-Hot Encoding\n",
        "\n",
        "# Model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), padding='same', strides=(1,1), activation='relu', input_shape=(32,32,3)))\n",
        "model.add(pooling.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(64, (3,3), padding='same', strides=(1,1), activation='relu'))\n",
        "model.add(pooling.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='softmax')) # units=10, activation='softmax'\n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Training\n",
        "model.fit(train_X, train_Y, epochs=25)\n",
        "# Testing\n",
        "_, accuracy = model.evaluate(test_X, test_Y)\n",
        "print('Accuracy: ', accuracy)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n",
            "170508288/170498071 [==============================] - 4s 0us/step\n",
            "(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                40970     \n",
            "=================================================================\n",
            "Total params: 60,362\n",
            "Trainable params: 60,362\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 1.9539 - accuracy: 0.3034\n",
            "Epoch 2/25\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 1.5859 - accuracy: 0.4398\n",
            "Epoch 3/25\n",
            "1563/1563 [==============================] - 68s 44ms/step - loss: 1.4231 - accuracy: 0.4959\n",
            "Epoch 4/25\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 1.3199 - accuracy: 0.5348\n",
            "Epoch 5/25\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 1.2443 - accuracy: 0.5631\n",
            "Epoch 6/25\n",
            "1563/1563 [==============================] - 68s 44ms/step - loss: 1.1806 - accuracy: 0.5899\n",
            "Epoch 7/25\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 1.1233 - accuracy: 0.6108\n",
            "Epoch 8/25\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 1.0740 - accuracy: 0.6290\n",
            "Epoch 9/25\n",
            "1563/1563 [==============================] - 71s 45ms/step - loss: 1.0305 - accuracy: 0.6469\n",
            "Epoch 10/25\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 0.9948 - accuracy: 0.6567\n",
            "Epoch 11/25\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 0.9618 - accuracy: 0.6681\n",
            "Epoch 12/25\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 0.9344 - accuracy: 0.6783\n",
            "Epoch 13/25\n",
            "1563/1563 [==============================] - 68s 44ms/step - loss: 0.9081 - accuracy: 0.6864\n",
            "Epoch 14/25\n",
            "1563/1563 [==============================] - 66s 43ms/step - loss: 0.8831 - accuracy: 0.6965\n",
            "Epoch 15/25\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.8626 - accuracy: 0.7038\n",
            "Epoch 16/25\n",
            "1563/1563 [==============================] - 66s 42ms/step - loss: 0.8423 - accuracy: 0.7105\n",
            "Epoch 17/25\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.8260 - accuracy: 0.7171\n",
            "Epoch 18/25\n",
            "1563/1563 [==============================] - 68s 44ms/step - loss: 0.8067 - accuracy: 0.7232\n",
            "Epoch 19/25\n",
            "1563/1563 [==============================] - 66s 42ms/step - loss: 0.7901 - accuracy: 0.7288\n",
            "Epoch 20/25\n",
            "1563/1563 [==============================] - 66s 42ms/step - loss: 0.7743 - accuracy: 0.7352\n",
            "Epoch 21/25\n",
            "1563/1563 [==============================] - 65s 42ms/step - loss: 0.7589 - accuracy: 0.7401\n",
            "Epoch 22/25\n",
            "1563/1563 [==============================] - 65s 42ms/step - loss: 0.7435 - accuracy: 0.7445\n",
            "Epoch 23/25\n",
            "1563/1563 [==============================] - 65s 41ms/step - loss: 0.7283 - accuracy: 0.7517\n",
            "Epoch 24/25\n",
            "1563/1563 [==============================] - 65s 42ms/step - loss: 0.7149 - accuracy: 0.7551\n",
            "Epoch 25/25\n",
            "1563/1563 [==============================] - 65s 42ms/step - loss: 0.7024 - accuracy: 0.7599\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.9364 - accuracy: 0.6814\n",
            "Accuracy:  0.6814000010490417\n"
          ]
        }
      ]
    }
  ]
}